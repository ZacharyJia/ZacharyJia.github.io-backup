title: 机器学习笔记Coursera——Week 1
date: 2015-12-27 22:19:48
tags: 机器学习
---

最近准备跟进Coursera上Andrew Ng的Machine Learning课程，为自己打点Machine Learning的基础。课程时间是在2015-12-28~2016-3-21，正好在假期期间可以拿出时间静下心来学习。
学习的一些笔记和心得体会就放在这里了。

<!-- more -->

今天主要把第一周的内容大体上学习了一遍，了解了一下基本的概念和线性回归的一些内容。

主要的内容有以下几点：
1. 线性回归（Linear Regression）
2. 代价函数(Cost Function)
3. 梯度下降算法(Gradient Descent)

## 线性回归
线性回归是监督学习（Supervised Learning）的一种方式，通过对数据集的计算，得到最优的线性回归函数。
其基本的单变量线性回归公式为：
![公式](http://latex.codecogs.com/gif.download?h_%7B%5Ctheta%7D%28x%29%20%3D%20%5Ctheta_0%20+%20%5Ctheta_1%20%5Ctimes%20x)

通过对数据集的拟合，得到θ0和θ1。

## 代价函数
代价函数的公式为
`J(θ) = （方差） / 2m`
其中， 方差是线性回归函数的预期值与实际值之间的方差，m为数据集的个数。
代价函数代表了回归函数与数据集的拟合程度。代价函数的值越接近0， 则表示当前的θ0和θ1所对应的你和函数与数据集的拟合程度越好。

## 梯度下降算法
梯度下降算法的基本思想是从一个初始点出发，通过梯度，寻找J(θ)下降最快的方向，然后不断进行迭代，知道找到一个局部最优解或全局最优解。

### 特点：
梯度下降算法的特点是，从不同的初始点出发，可能会沿着不同的下降路径，得到不同的局部最优解。

### 定义公式：
![公式](http://latex.codecogs.com/gif.download?%5Ctheta_j%20%3A%3D%20%5Ctheta_j%20-%20%5Calpha%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Ctheta_j%7D%20J%28%5Ctheta_0%2C%20%5Ctheta_1%29)

其中，`:=`符号为赋值符号，`α`为学习速率（Learning rate）。

当α过小时，需要迭代很多次才能够找到局部最优解。
当α过大时，可能会跳过局部最优解，甚至会无法收敛。

但是，当α 固定时，梯度下降算法会自动减小迭代的步幅（因为导数值在减小），使其逐步接近局部最优解，因此不用特意减小α 的值。